{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc8e8a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'scraped_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 108\u001b[0m\n\u001b[0;32m    104\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscraped_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    105\u001b[0m fieldnames \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct URL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct Name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct Price\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRating\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Reviews\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    106\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mASIN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct Description\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mManufacturer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m    109\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(file, fieldnames\u001b[38;5;241m=\u001b[39mfieldnames)\n\u001b[0;32m    110\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriteheader()\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'scraped_data.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Function to scrape product information\n",
    "def scrape_product_info(url):\n",
    "    if not url.startswith('http'):\n",
    "        url = 'https://www.amazon.in' + url\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Initialize variables with default values\n",
    "    product_name = ''\n",
    "    product_price = ''\n",
    "    rating = ''\n",
    "    review_count = ''\n",
    "    description = ''\n",
    "    asin = ''\n",
    "    product_description = ''\n",
    "    manufacturer = ''\n",
    "\n",
    "    # Extract the product name if available\n",
    "    product_name_elem = soup.find('span', class_='a-size-medium')\n",
    "    if product_name_elem:\n",
    "        product_name = product_name_elem.text.strip()\n",
    "\n",
    "    # Extract the product price if available\n",
    "    product_price_elem = soup.find('span', class_='a-offscreen')\n",
    "    if product_price_elem:\n",
    "        product_price = product_price_elem.text.strip()\n",
    "\n",
    "    # Extract the rating if available\n",
    "    rating_elem = soup.find('span', class_='a-icon-alt')\n",
    "    if rating_elem:\n",
    "        rating = rating_elem.text.strip()\n",
    "\n",
    "    # Extract the number of reviews if available\n",
    "    review_count_elem = soup.find('span', class_='a-size-base')\n",
    "    if review_count_elem:\n",
    "        review_count = review_count_elem.text.strip()\n",
    "\n",
    "    # Extract additional product information if available\n",
    "    description_elem = soup.select_one('#productDescription p')\n",
    "    if description_elem:\n",
    "        description = description_elem.text.strip()\n",
    "\n",
    "    asin_elem = soup.select_one('#prodDetails th:-soup-contains(\"ASIN\") + td')\n",
    "    if asin_elem:\n",
    "        asin = asin_elem.text.strip()\n",
    "\n",
    "    product_description_elem = soup.select_one('#productDescription .product-description-content')\n",
    "    if product_description_elem:\n",
    "        product_description = product_description_elem.text.strip()\n",
    "\n",
    "    manufacturer_elem = soup.select_one('#bylineInfo a')\n",
    "    if manufacturer_elem:\n",
    "        manufacturer = manufacturer_elem.text.strip()\n",
    "\n",
    "    # Create a dictionary with the scraped data\n",
    "    data = {\n",
    "        'Product URL': url,\n",
    "        'Product Name': product_name,\n",
    "        'Product Price': product_price,\n",
    "        'Rating': rating,\n",
    "        'Number of Reviews': review_count,\n",
    "        'Description': description,\n",
    "        'ASIN': asin,\n",
    "        'Product Description': product_description,\n",
    "        'Manufacturer': manufacturer\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "    \n",
    "\n",
    "# Function to scrape product listing pages\n",
    "def scrape_product_listing_pages(start_url, num_pages):\n",
    "    all_data = []\n",
    "\n",
    "    for page in range(1, num_pages + 1):\n",
    "        url = f'{start_url}&page={page}'\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract product URLs\n",
    "        product_urls = [item['href'] for item in soup.select('a.a-link-normal')]\n",
    "\n",
    "        for product_url in product_urls:\n",
    "            # Scrape product information\n",
    "            data = scrape_product_info(product_url)\n",
    "            all_data.append(data)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Define the start URL and number of pages to scrape\n",
    "start_url = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1'\n",
    "num_pages = 20\n",
    "\n",
    "# Scrape product listing pages\n",
    "scraped_data = scrape_product_listing_pages(start_url, num_pages)\n",
    "scraped_data\n",
    "# Write the data to a CSV file\n",
    "csv_file = 'scraped_data.csv'\n",
    "fieldnames = ['Product URL', 'Product Name', 'Product Price', 'Rating', 'Number of Reviews',\n",
    "              'Description', 'ASIN', 'Product Description', 'Manufacturer']\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(scraped_data)\n",
    "\n",
    "print(f'Scraped data has been saved to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64554df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
